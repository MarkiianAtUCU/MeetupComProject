{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "import json\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0,com.datastax.spark:spark-cassandra-connector_2.11:2.5.0,com.github.jnr:jffi:1.2.19 pyspark-shell'\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql.functions import col, struct, lit\n",
    "import datetime\n",
    "import calendar\n",
    "from collections import Counter\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.\\\n",
    "      readStream.\\\n",
    "      format(\"kafka\").\\\n",
    "      option(\"kafka.bootstrap.servers\", \"localhost:9092\").\\\n",
    "      option(\"subscribe\", \"raw-meetups\").\\\n",
    "      option(\"startingOffsets\", \"earliest\").\\\n",
    "      load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct = T.StructType([\n",
    "    T.StructField('venue', T.StructType([\n",
    "        T.StructField(\"venue_name\", T.StringType()),\n",
    "        T.StructField(\"lon\", T.FloatType()),\n",
    "        T.StructField(\"lat\", T.FloatType()),\n",
    "        T.StructField(\"venue_id\", T.IntegerType())\n",
    "    ])),\n",
    "    T.StructField(\"visibility\", T.StringType()),\n",
    "    T.StructField(\"response\", T.StringType()),\n",
    "    T.StructField(\"guests\", T.IntegerType()),\n",
    "    T.StructField('member', T.StructType([\n",
    "        T.StructField(\"member_id\", T.IntegerType()),\n",
    "        T.StructField(\"photo\", T.StringType()),\n",
    "        T.StructField(\"member_name\", T.StringType())\n",
    "    ])),\n",
    "    T.StructField(\"rsvp_id\", T.IntegerType()),\n",
    "    T.StructField(\"mtime\", T.LongType()),\n",
    "    T.StructField('event', T.StructType([\n",
    "        T.StructField(\"event_name\", T.StringType()),\n",
    "        T.StructField(\"event_id\", T.StringType()),\n",
    "        T.StructField(\"time\", T.LongType()),\n",
    "        T.StructField(\"event_url\", T.StringType())\n",
    "    ])),\n",
    "    T.StructField('group', T.StructType([\n",
    "        T.StructField(\"group_topics\", T.ArrayType(T.StructType([\n",
    "            T.StructField(\"urlkey\", T.StringType()),\n",
    "            T.StructField(\"topic_name\", T.StringType())\n",
    "        ]))),\n",
    "        T.StructField(\"group_city\", T.StringType()),\n",
    "        T.StructField(\"group_country\", T.StringType()),\n",
    "        T.StructField(\"group_id\", T.IntegerType()),\n",
    "        T.StructField(\"group_name\", T.StringType()),\n",
    "        T.StructField(\"group_lon\", T.FloatType()),\n",
    "        T.StructField(\"group_urlname\", T.StringType()),\n",
    "        T.StructField(\"group_state\", T.StringType()),\n",
    "        T.StructField(\"group_lat\", T.FloatType())\n",
    "    ]))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_names = spark.read.json(\"data/USstate.json\")\n",
    "countries_names = spark.read.json(\"data/Countries.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_parsed_df = df.select(\n",
    "    col('timestamp'),\n",
    "    F.from_json(col(\"value\").cast(\"string\"), struct).alias(\"json_parsed\")\n",
    ").select(\n",
    "    'timestamp',\n",
    "    col('json_parsed.event.event_id'),\n",
    "    col('json_parsed.event.event_name'),\n",
    "    col('json_parsed.event.time'),\n",
    "    col('json_parsed.group.group_id'),\n",
    "    col('json_parsed.group.group_country'),\n",
    "    col('json_parsed.group.group_state'),\n",
    "    col('json_parsed.group.group_city'),\n",
    "    col('json_parsed.group.group_topics.topic_name'),\n",
    "    col('json_parsed.group.group_name')\n",
    ").join(countries_names, col(\"group_country\") == countries_names.country_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f1a296cc4e0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TASK A_1 \n",
    "json_parsed_df.withWatermark(\"timestamp\", \"1 minute\").groupBy(\n",
    "    F.window(\"timestamp\", \"1 minute\", \"1 minute\"), 'country_name'\n",
    ").agg(\n",
    "    F.count('country_name').alias('count')\n",
    ").select(\n",
    "    F.struct(\n",
    "        col('window.start').alias(\"datetime_start\"), \n",
    "        col('window.end').alias(\"datetime_end\"), \n",
    "        F.create_map([\"country_name\",\"count\"]).alias(\"map_item\")    \n",
    "    ).alias(\"res\")\n",
    ").select(F.to_json('res').alias('value')).writeStream\\\n",
    ".format(\"kafka\") \\\n",
    ".option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    ".option(\"topic\", \"tt8-meetups\") \\\n",
    ".option(\"checkpointLocation\", \"home/mmatsi/tmp/US-meetups_topick_ckp_aaaaaaa\") \\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_maps(r):\n",
    "    res = dict()\n",
    "    for i in r:\n",
    "        key = next(iter(i))\n",
    "        if key in res:\n",
    "            res[key] += i[key]\n",
    "        else:\n",
    "            res[key] = i[key]\n",
    "\n",
    "    return [{key: res[key]} for key in res]\n",
    "\n",
    "sum_maps_udf = F.udf(sum_maps, T.ArrayType( T.MapType(T.StringType(), T.IntegerType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f1a297315f8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TASK A_1_res\n",
    "a1_struct = T.StructType([\n",
    "    T.StructField(\"datetime_start\", T.TimestampType()),\n",
    "    T.StructField(\"datetime_end\", T.TimestampType()),\n",
    "    T.StructField(\"map_item\", T.MapType(T.StringType(), T.IntegerType())),\n",
    "])\n",
    "\n",
    "a1_json_parsed_df = spark.\\\n",
    "      readStream.\\\n",
    "      format(\"kafka\").\\\n",
    "      option(\"kafka.bootstrap.servers\", \"localhost:9092\").\\\n",
    "      option(\"subscribe\", \"tt8-meetups\").\\\n",
    "      option(\"startingOffsets\", \"earliest\").\\\n",
    "      load().select(\n",
    "            F.from_json(col(\"value\").cast(\"string\"), a1_struct).alias(\"json_parsed\")\n",
    "      ).select(\"json_parsed.*\")\n",
    "\n",
    "a1_json_parsed_df.withWatermark(\"datetime_end\", \"1 minute\").groupBy(\n",
    "    F.window(\"datetime_end\", \"6 minute\", \"1 minute\")\n",
    ")\\\n",
    ".agg(\n",
    "    F.first(\"window.start\").alias(\"timestamp_start\"),\n",
    "    F.first(\"window.end\").alias(\"timestamp_end\"),  \n",
    "    F.collect_list(\"map_item\").alias(\"statistics\")\n",
    ").select(\n",
    "    F.struct(\n",
    "         F.concat(F.hour('timestamp_start'),lit(\":\"),F.minute('timestamp_start')).alias(\"time_start\"),\n",
    "         F.concat(F.hour('timestamp_end'),lit(\":\"),F.minute('timestamp_end')).alias(\"time_end\"),\n",
    "         col('timestamp_end').alias(\"time_end\"),\n",
    "         sum_maps_udf(col('statistics')).alias(\"statistics\")\n",
    "    ).alias(\"res\")\n",
    ").select(F.to_json('res').alias('value')).writeStream\\\n",
    ".format(\"kafka\") \\\n",
    ".option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    ".option(\"topic\", \"meetups-by-country1\") \\\n",
    ".option(\"checkpointLocation\", \"home/mmatsi/tmp/US-meetups_topick_ckp_bbb\") \\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f3a33c952b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TASK A_2\n",
    "json_parsed_df.filter(col('group_country')=='us').withColumn('topic_exploded', F.explode('topic_name')).withWatermark(\"timestamp\", \"1 minute\").groupBy(\n",
    "    F.window(\"timestamp\", \"1 minute\", \"1 minute\"), 'group_state'\n",
    ").agg(\n",
    "    F.collect_set('topic_exploded').alias('topic_list')\n",
    ").join(states_names, col(\"group_state\") == states_names.code).select(\n",
    "    F.struct(\n",
    "        col('window.start').alias(\"datetime_start\"), \n",
    "        col('window.end').alias(\"datetime_end\"), \n",
    "        F.create_map([\"state_name\",\"topic_list\"]).alias(\"map_topics\")   \n",
    "    ).alias(\"res\")\n",
    ").select(F.to_json('res').alias('value')).writeStream\\\n",
    ".format(\"kafka\") \\\n",
    ".option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    ".option(\"topic\", \"topics-by-state_preparation1\") \\\n",
    ".option(\"checkpointLocation\", \"home/mmatsi/tmp/topics-by-stlate_preparation\") \\\n",
    ".start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_maps(r):\n",
    "    res = dict()\n",
    "    for i in r:\n",
    "        key = next(iter(i))\n",
    "        if key in res:\n",
    "            res[key] += i[key]\n",
    "        else:\n",
    "            res[key] = i[key]\n",
    "\n",
    "    return [{key: list(set(res[key]))} for key in res]\n",
    "\n",
    "concat_maps_udf = F.udf(concat_maps, T.ArrayType( T.MapType(\n",
    "        T.StringType(), \n",
    "        T.ArrayType(T.StringType())\n",
    "    )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f3a33cb34a8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TASK A_2_res\n",
    "a2_struct = T.StructType([\n",
    "    T.StructField(\"datetime_start\", T.TimestampType()),\n",
    "    T.StructField(\"datetime_end\", T.TimestampType()),\n",
    "    T.StructField(\"map_topics\", T.MapType(\n",
    "        T.StringType(), \n",
    "        T.ArrayType(T.StringType())\n",
    "    ))\n",
    "])\n",
    "\n",
    "a2_json_parsed_df = spark.\\\n",
    "      readStream.\\\n",
    "      format(\"kafka\").\\\n",
    "      option(\"kafka.bootstrap.servers\", \"localhost:9092\").\\\n",
    "      option(\"subscribe\", \"topics-by-state_preparation1\").\\\n",
    "      option(\"startingOffsets\", \"earliest\").\\\n",
    "      load().select(\n",
    "            F.from_json(col(\"value\").cast(\"string\"), a2_struct).alias(\"json_parsed\")\n",
    "      ).select(\"json_parsed.*\")\n",
    "\n",
    "a2_json_parsed_df.withWatermark(\"datetime_end\", \"1 minute\").groupBy(\n",
    "    F.window(\"datetime_end\", \"3 minute\", \"1 minute\")\n",
    ")\\\n",
    ".agg(\n",
    "    F.first(\"window.start\").alias(\"timestamp_start\"),\n",
    "    F.first(\"window.end\").alias(\"timestamp_end\"),  \n",
    "    F.collect_list(\"map_topics\").alias(\"statistics\")\n",
    ")\\\n",
    ".select(\n",
    "    F.struct(\n",
    "         F.concat(F.hour('timestamp_start'),lit(\":\"),F.minute('timestamp_start')).alias(\"time_start\"),\n",
    "         F.concat(F.hour('timestamp_end'),lit(\":\"),F.minute('timestamp_end')).alias(\"time_end\"),\n",
    "         concat_maps_udf(col('statistics')).alias(\"statistics\")\n",
    "    ).alias(\"res\")\n",
    ")\\\n",
    "# .writeStream.format(\"console\").start()\n",
    ".select(F.to_json('res').alias('value')).writeStream\\\n",
    ".format(\"kafka\") \\\n",
    ".option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    ".option(\"topic\", \"topics-by-state2\") \\\n",
    ".option(\"checkpointLocation\", \"home/mmatsi/tmp/US-meetupsl_topkklick_cfkp_bbsdfb\") \\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f4293d37940>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TASK A_3\n",
    "json_parsed_df.withColumn('topic_name_exp', F.explode('topic_name')).withWatermark(\"timestamp\", \"1 minute\").groupBy(\n",
    "    F.window(\"timestamp\", \"1 minute\", \"1 minute\"), 'country_name', 'topic_name_exp'\n",
    ").agg(\n",
    "    F.count('topic_name_exp').alias('topic_count')\n",
    ").select(\n",
    "    F.struct(\n",
    "        col('window.end').alias(\"datetime_end\"), \n",
    "        col('country_name'),\n",
    "        col('topic_name_exp'),\n",
    "        col('topic_count')\n",
    "    ).alias(\"res\")\n",
    ").select(F.to_json('res').alias('value')).writeStream\\\n",
    ".format(\"kafka\") \\\n",
    ".option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    ".option(\"topic\", \"topics-by-country_prefetching\") \\\n",
    ".option(\"checkpointLocation\", \"home/mmatsi/tmp/topics-by-stlaste_preparation\") \\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f4293c9c3c8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TASK A_3_proc\n",
    "a3_struct = T.StructType([\n",
    "    T.StructField(\"datetime_end\", T.TimestampType()),\n",
    "    T.StructField(\"country_name\", T.StringType()),\n",
    "    T.StructField(\"topic_name_exp\", T.StringType()),\n",
    "    T.StructField(\"topic_count\", T.IntegerType()),\n",
    "])\n",
    "\n",
    "a3_json_parsed_df = spark.\\\n",
    "      readStream.\\\n",
    "      format(\"kafka\").\\\n",
    "      option(\"kafka.bootstrap.servers\", \"localhost:9092\").\\\n",
    "      option(\"subscribe\", \"topics-by-country_prefetching\").\\\n",
    "      option(\"startingOffsets\", \"earliest\").\\\n",
    "      load().select(\n",
    "            F.from_json(col(\"value\").cast(\"string\"), a3_struct).alias(\"json_parsed\")\n",
    "      ).select(\"json_parsed.*\")\n",
    "\n",
    "a3_json_parsed_df.withWatermark(\"datetime_end\", \"1 minute\").groupBy(\n",
    "    F.window(\"datetime_end\", \"6 minute\", \"1 minute\"), 'country_name', 'topic_name_exp'\n",
    ")\\\n",
    ".agg(\n",
    "    F.sum('topic_count').alias(\"topic_sum\")\n",
    ").select(\n",
    "    F.struct(\n",
    "        col(\"window.start\").alias(\"timetamp_start\"),\n",
    "        col(\"window.end\").alias(\"timetamp_end\"),\n",
    "        col(\"country_name\"),\n",
    "        col(\"topic_name_exp\"),\n",
    "        col(\"topic_sum\")    \n",
    "    ).alias(\"res\")\n",
    ").select(F.to_json('res').alias('value')).writeStream\\\n",
    ".format(\"kafka\") \\\n",
    ".option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    ".option(\"topic\", \"topics-by-country_processed\") \\\n",
    ".option(\"checkpointLocation\", \"home/mmatsi/tmp/topics-by-stlastke_preparation\") \\\n",
    ".start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f4293f84cf8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TASK A_3_postprocessing\n",
    "a3_struct_res = T.StructType([\n",
    "    T.StructField(\"timetamp_start\", T.TimestampType()),\n",
    "    T.StructField(\"timetamp_end\", T.TimestampType()),\n",
    "    T.StructField(\"country_name\", T.StringType()),\n",
    "    T.StructField(\"topic_name_exp\", T.StringType()),\n",
    "    T.StructField(\"topic_sum\", T.IntegerType()),\n",
    "])\n",
    "\n",
    "a3_res_json_parsed_df = spark.\\\n",
    "      readStream.\\\n",
    "      format(\"kafka\").\\\n",
    "      option(\"kafka.bootstrap.servers\", \"localhost:9092\").\\\n",
    "      option(\"subscribe\", \"topics-by-country_processed\").\\\n",
    "      option(\"startingOffsets\", \"earliest\").\\\n",
    "      load().select(\n",
    "            F.from_json(col(\"value\").cast(\"string\"), a3_struct_res).alias(\"json_parsed\")\n",
    "      ).select(\"json_parsed.*\")\n",
    "\n",
    "a3_res_json_parsed_df.withColumn(\"topic_map\", F.struct(\n",
    "        col('topic_sum'),\n",
    "        col('topic_name_exp'),\n",
    ")).withWatermark(\"timetamp_start\", \"1 minute\").groupBy(\n",
    "    \"timetamp_start\", \"timetamp_end\", \"country_name\"\n",
    ").agg(\n",
    "    F.max(\"topic_map\").alias(\"max_map\")\n",
    ").select(\n",
    "    F.struct(\n",
    "        col('timetamp_start'),\n",
    "        col('timetamp_end'),\n",
    "        col('country_name'),\n",
    "        col('max_map.topic_name_exp').alias(\"topic_name_exp\"),\n",
    "        col('max_map.topic_sum').alias(\"topic_sum\")\n",
    "    ).alias(\"res\")\n",
    ").select(F.to_json('res').alias('value')).writeStream\\\n",
    ".format(\"kafka\") \\\n",
    ".option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    ".option(\"topic\", \"topics-by-country_postprocess2\") \\\n",
    ".option(\"checkpointLocation\", \"home/mmatsi/tmp/topics-by-stllkastke_preparation\") \\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f4293ca30f0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a3_res_json_parsed_df = spark.\\\n",
    "      readStream.\\\n",
    "      format(\"kafka\").\\\n",
    "      option(\"kafka.bootstrap.servers\", \"localhost:9092\").\\\n",
    "      option(\"subscribe\", \"topics-by-country_postprocess2\").\\\n",
    "      option(\"startingOffsets\", \"earliest\").\\\n",
    "      load().select(\n",
    "            F.from_json(col(\"value\").cast(\"string\"), a3_struct_res).alias(\"json_parsed\")\n",
    "      ).select(\"json_parsed.*\")\n",
    "\n",
    "a3_res_json_parsed_df.withWatermark(\"timetamp_start\", \"1 minute\").groupBy(\n",
    "    \"timetamp_start\", \"timetamp_end\"\n",
    ").agg(\n",
    "    F.collect_list(F.create_map([\"country_name\",\"topic_sum\"])).alias(\"statistics\")\n",
    ").select(\n",
    "    F.struct(\n",
    "        F.concat(F.hour('timetamp_start'),lit(\":\"),F.minute('timetamp_start')).alias(\"time_start\"),\n",
    "        F.concat(F.hour('timetamp_end'),lit(\":\"),F.minute('timetamp_end')).alias(\"time_end\"),\n",
    "        col('statistics')\n",
    "    ).alias(\"res\")\n",
    ").select(F.to_json('res').alias('value')).writeStream\\\n",
    ".format(\"kafka\") \\\n",
    ".option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    ".option(\"topic\", \"topics-by-country\") \\\n",
    ".option(\"checkpointLocation\", \"home/mmatsi/tmp/topics-bykk-stlkastke_preparation\") \\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hosts = {\n",
    "    \"spark.cassandra.connection.host\": '34.240.57.255',\n",
    "    \"spark.cassandra.auth.username\": 'cassandra',\n",
    "    \"spark.cassandra.auth.password\": 'cassandra'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fb557ce5278>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TASK_B_1\n",
    "json_parsed_df.select(\n",
    "    col('country_name')\n",
    ").writeStream.format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".option(\"checkpointLocation\", \"kkk1\")\\\n",
    ".options(**hosts)\\\n",
    ".option(\"keyspace\", \"meetup_net_project\")\\\n",
    ".option(\"table\", \"countries_set\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fb577d3a4a8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TASK_B_2\n",
    "json_parsed_df.select(\n",
    "    col('country_name'),\n",
    "    col('group_city'),\n",
    ").writeStream\\\n",
    ".format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".option(\"checkpointLocation\", \"kkk3\")\\\n",
    ".options(**hosts)\\\n",
    ".option(\"keyspace\", \"meetup_net_project\")\\\n",
    ".option(\"table\", \"cities_by_country\").start()\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fb557cee940>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TASK_B_3\n",
    "json_parsed_df.select(\n",
    "    'event_id',\n",
    "    'event_name',\n",
    "     F.from_unixtime(col('time') / 1000).alias('event_time'),\n",
    "    col('topic_name').alias(\"topics\"),\n",
    "    col('group_name'),\n",
    "    col('country_name'),\n",
    "    col('group_city')\n",
    ").writeStream.format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".option(\"checkpointLocation\", \"kkk2\")\\\n",
    ".options(**hosts)\\\n",
    ".option(\"keyspace\", \"meetup_net_project\")\\\n",
    ".option(\"table\", \"event_by_id\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fb557ce5fd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TASK_B_4\n",
    "json_parsed_df.select(\n",
    "    col('group_city'), \n",
    "    col('group_name'),\n",
    "    col('group_id')\n",
    ").writeStream.format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".option(\"checkpointLocation\", \"kkk5\")\\\n",
    ".options(**hosts)\\\n",
    ".option(\"keyspace\", \"meetup_net_project\")\\\n",
    ".option(\"table\", \"groups_by_city\")\\\n",
    ".start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fb577d3af98>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TASK_B_5\n",
    "json_parsed_df.select(\n",
    "    col('group_id'),\n",
    "    col('event_id')\n",
    ").writeStream.format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".option(\"checkpointLocation\", \"kkk6\")\\\n",
    ".options(**hosts)\\\n",
    ".option(\"keyspace\", \"meetup_net_project\")\\\n",
    ".option(\"table\", \"events_by_group\")\\\n",
    ".start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (spark_lab)",
   "language": "python",
   "name": "pycharm-65440466"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}